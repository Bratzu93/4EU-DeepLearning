{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SydanJainen/4EU-DeepLearningCourse/blob/main/Assignments/AssignementCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMteOzeJ8hrL"
   },
   "source": [
    "# Assignments\n",
    "\n",
    "### We are given a dataset containing 510 images which are categorized according two main characteristics/properties: “C1” and “C2”. The following inputs are available:\n",
    "* #### `input image', a 110 × 110 × 3 real-valued tensor. The last dimension denotes the number of input channels; the images belong to different combinations of the two properties C1 and C2. For C1 and C2 we have 5 and 6 possible values, respectively;  among the 30 possible couples (C1,C2), we have images for only 24 of them. We have around 20 samples (little more or less) for each of the available couples;  values in each tensor entry are integers in [0, 255];\n",
    "* #### C1, string, the value for the C1 property. 5 possible values;\n",
    "* #### C2, string, the value for the C2 property. 6 possible values.\n",
    "\n",
    "### Design a deep neural network model to predict the class of an image, given by the couple (C1,C2).\n",
    "\n",
    "### Provide a sketch of each of the following points, then implement your solution.\n",
    "1. MODEL: Which architecture do you consider the most appropriate for this task, and why;\n",
    "2. INPUT:\n",
    "* After a potential preprocessing step, which is the input of the model, and\n",
    "how is it represented;\n",
    "3. OUTPUT: How would you design the output layer and why;\n",
    "4. LOSS: Which loss function would you use to train your model and why;\n",
    "5. MODEL CONFIGURATION:\n",
    " Model composition (composition of layers, regardless their number,\n",
    "or their dimension, which can be object of tuning)\n",
    "6. MODEL EVALUATION: How would you assess (in which setting) the\n",
    "generalization capabilities of the model on unseen data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OI9U1PiwZ8EJ",
    "outputId": "4706d2d4-1e8a-4bea-989b-c374e7414506"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m, force_remount\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m## go to your datapath\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "## go to your datapath\n",
    "%cd /content/drive/MyDrive/INSEGNAMENTI/4EU+/\n",
    "\n",
    "# Here you should see the desired files\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApgM0K_w8hrQ",
    "outputId": "d2a1bd24-c33c-4c3a-a41c-a55e6baca7f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length image:510\n",
      "length C1:510\n",
      "length C2:510\n"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "# Open the pickle data\n",
    "with open('input.pkl', 'rb') as f:\n",
    "  data = pk.load(f)\n",
    "\n",
    "images = data['imgs']\n",
    "C1 = data['C1']\n",
    "C2 = data['C2']\n",
    "print(f\"length image:{len(images)}\")\n",
    "print(f\"length C1:{len(C1)}\")\n",
    "print(f\"length C2:{len(C2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "E5t2r4xPY7xy"
   },
   "outputs": [],
   "source": [
    "#import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sPKEIxDxbLzw"
   },
   "outputs": [],
   "source": [
    "transofm = Compose([ToTensor(),\n",
    "                        Normalize((0,), (1,))])\n",
    "tensor_images = []\n",
    "for i in images :\n",
    "  tensor_images.append(transofm(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_9ZK3RfIRE9",
    "outputId": "9460ab5f-29f1-40d3-d9f8-7cf7af4cb136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dress\n",
      "pants\n",
      "shirt\n",
      "shoes\n",
      "shorts\n"
     ]
    }
   ],
   "source": [
    "for a in np.unique(C1):\n",
    "  print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3o2q8J0jKzR5",
    "outputId": "46a1cc0f-4b2d-48f1-f684-ee992b3890ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black\n",
      "blue\n",
      "brown\n",
      "green\n",
      "red\n",
      "white\n"
     ]
    }
   ],
   "source": [
    "for a in np.unique(C2):\n",
    "  print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzlpo6nuPlX4",
    "outputId": "32dc12a5-ccc9-4208-c721-083f515de153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shoes white': 0, 'pants white': 1, 'pants blue': 2, 'shoes red': 3, 'shoes black': 4, 'shorts brown': 5, 'shorts white': 6, 'shoes green': 7, 'pants red': 8, 'dress red': 9, 'shirt green': 10, 'dress blue': 11, 'shirt black': 12, 'pants black': 13, 'pants brown': 14, 'dress white': 15, 'pants green': 16, 'shorts green': 17, 'shoes brown': 18, 'dress black': 19, 'shoes blue': 20, 'shorts black': 21, 'shirt blue': 22, 'shorts blue': 23}\n",
      "510\n"
     ]
    }
   ],
   "source": [
    "main_dataset = zip(tensor_images, C1, C2)\n",
    "\n",
    "label_combinations = {}\n",
    "for i, (c1, c2) in enumerate(zip(C1, C2)):\n",
    "    label_combinations[i] = f\"{c1} {c2}\"\n",
    "\n",
    "unique_labels = set(label_combinations.values())\n",
    "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "print(label_to_index)\n",
    "\n",
    "enumerated_dataset = []\n",
    "for i, (tensor_images, c1, c2) in enumerate(zip(tensor_images, C1, C2)):\n",
    "  label_combination = f\"{c1} {c2}\"\n",
    "  label_index = label_to_index[label_combination]\n",
    "  enumerated_dataset.append((tensor_images, label_index))\n",
    "\n",
    "print(len(enumerated_dataset))\n",
    "#print(len(main_dataset))\n",
    "\n",
    "#one_hot = torch.nn.functional.one_hot(enumerated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LwPZKxDbZQb2"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "\n",
    "train_size = int(0.7 * len(enumerated_dataset))\n",
    "val_size = int(0.1 * len(enumerated_dataset))\n",
    "test_size = len(enumerated_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(enumerated_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pMz5W88eDujv"
   },
   "outputs": [],
   "source": [
    "def dimension(N,P,S,F) :\n",
    "  return (int((N + (2*P) - F)/ S) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vUdueaJfuPCC"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, kernel_size, stride, padding, pooling_strategy, dropout_rate):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=32,\n",
    "                               kernel_size = kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=padding)\n",
    "        dim1 = dimension(110, padding, stride, kernel_size)\n",
    "        #print(dim1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size, stride, padding)\n",
    "        dim2 = dimension(dim1, padding, stride, kernel_size)\n",
    "        #print(dim2)\n",
    "        if pooling_strategy == \"mean\" :\n",
    "          self.pool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        elif pooling_strategy == \"max\" :\n",
    "          self.pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        else :\n",
    "          raise Exception()\n",
    "        dim3 = int(((dim2-2)/2 +1) * ((dim2-2)/2 +1) * 64) # 'same' = dim3\n",
    "        #print(dim3)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(dim3, 128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2d(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "kernel_size_list = [3, 4, 5]\n",
    "stride = 1\n",
    "padding = 0\n",
    "pooling_strategy_list = ['mean', 'max']\n",
    "dropout_rate_list = [0, 0.1, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "KswJYORFuor3"
   },
   "outputs": [],
   "source": [
    "# Computing the accuracy of a test set\n",
    "def evaluate(model, test_loader, error):\n",
    "#model = mlpinplace=False\n",
    "    correct = 0\n",
    "    cur_loss = 0;\n",
    "\n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        test_imgs, test_labels = test_imgs.cuda(), test_labels.cuda()\n",
    "        output = model(test_imgs)\n",
    "        loss = error(output, test_labels)\n",
    "        cur_loss+=loss.item()\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    return cur_loss/len(test_loader.dataset), correct/len(test_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "def fit(model, train_loader, error, test_loader, epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)#,lr=0.001, betas=(0.9,0.999))\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        cur_loss = 0;\n",
    "        correct = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            if dev == \"cpu\":\n",
    "              X_batch, y_batch = X_batch.cpu(), y_batch.cpu()\n",
    "            else:\n",
    "              X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = error(output, y_batch)\n",
    "            cur_loss+= loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_losses.append(cur_loss/len(train_loader.dataset))\n",
    "\n",
    "        test_loss, _ = evaluate(model, test_loader, error)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f'Epoch : {epoch},  train loss:{train_losses[-1]}, test loss:{test_losses[-1]}')\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "L5WJ0nMU-Vxc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7qaXTRbZTJ6",
    "outputId": "0b52659a-c07f-477c-9271-e3f7c45c2757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel size:3, pooling:mean,                  dropout rate:0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m   model\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     27\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(dev)\n\u001b[1;32m---> 30\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m fit(model,train_loader, error, val_loader, epochs)\n\u001b[0;32m     32\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m evaluate(model, train_loader, error)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTrain Accuracy:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 41\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader, error, test_loader, epochs)\u001b[0m\n\u001b[0;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(cur_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset))\n\u001b[1;32m---> 41\u001b[0m test_loss, _ \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, error)\n\u001b[0;32m     42\u001b[0m test_losses\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,  train loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, test_loader, error)\u001b[0m\n\u001b[0;32m      5\u001b[0m cur_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m;\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_imgs, test_labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m----> 8\u001b[0m     test_imgs, test_labels \u001b[38;5;241m=\u001b[39m test_imgs\u001b[38;5;241m.\u001b[39mcuda(), test_labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      9\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(test_imgs)\n\u001b[0;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m error(output, test_labels)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "\n",
    "error = nn.CrossEntropyLoss()\n",
    "## We use validation data to select the best hyperparameters\n",
    "## Fewer epochs to speed-up\n",
    "epochs = 5\n",
    "### initializing best config\n",
    "best_accval = 0\n",
    "best_ks = -1\n",
    "best_pool = \"\"\n",
    "best_dr = -1\n",
    "for kernel_size in kernel_size_list:\n",
    "  for pooling_strategy in pooling_strategy_list:\n",
    "    for dropout_rate in dropout_rate_list:\n",
    "\n",
    "      model = CNN(kernel_size, stride, padding, pooling_strategy, dropout_rate)\n",
    "      print(f\"kernel size:{kernel_size}, pooling:{pooling_strategy},\\\n",
    "                  dropout rate:{dropout_rate}\")\n",
    "\n",
    "      it = iter(train_loader)\n",
    "      X_batch, y_batch = next(it)\n",
    "\n",
    "      if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "        model.cuda()\n",
    "      else:\n",
    "        dev = \"cpu\"\n",
    "        model.cpu()\n",
    "      device = torch.device(dev)\n",
    "\n",
    "\n",
    "      train_losses, test_losses = fit(model,train_loader, error, val_loader, epochs)\n",
    "\n",
    "      loss, acc = evaluate(model, train_loader, error)\n",
    "      print(f\"\\tTrain Accuracy:{acc}, Train loss:{loss}\")\n",
    "\n",
    "      loss, acc = evaluate(model, val_loader, error)\n",
    "      print(f\"\\tValidation Accuracy:{acc}, Validation loss:{loss}\")\n",
    "      if acc > best_accval:\n",
    "        best_accval = acc\n",
    "        best_ks = kernel_size\n",
    "        best_pool = pooling_strategy\n",
    "        best_dr = dropout_rate\n",
    "\n",
    "      print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "id": "hrMWNW17ZUd_",
    "outputId": "a346c5e3-2978-44df-c294-cfa07eac6879"
   },
   "outputs": [],
   "source": [
    "print(f\"best configuration: kernel size:{best_ks}, pooling:{best_pool},\\\n",
    "                  dropuout rate:{best_dr}\")\n",
    "# Pick the best model and verify its performance on the test set\n",
    "model = CNN(best_ks, stride, padding, best_pool, best_dr)\n",
    "\n",
    "#it = iter(train_dataloader)\n",
    "#X_batch, y_batch = next(it)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "model.cuda()\n",
    "## we set again more epochs for the final training.\n",
    "## Note that here we could integrate the validaiton data in the training set\n",
    "epochs = 15\n",
    "train_losses, test_losses = fit(model,train_loader, error, test_loader, epochs)\n",
    "\n",
    "loss, acc = evaluate(model, test_loader, error)\n",
    "print(f\"Test Accuracy:{acc}, Test loss:{loss}\")\n",
    "\n",
    "plt.plot(train_losses,label=\"training loss\")\n",
    "plt.plot(test_losses,label=\"testing loss\")\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.ylabel('Loss', fontsize=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIVAo80VZWpk",
    "outputId": "4a4cf480-6d25-4251-c756-d986203ba52b"
   },
   "outputs": [],
   "source": [
    "# validate and test\n",
    "\n",
    "loss, acc = evaluate(model, train_loader, error)\n",
    "print(f\"Train Accuracy:{acc}, Train loss:{loss}\")\n",
    "\n",
    "loss, acc = evaluate(model, test_loader, error)\n",
    "print(f\"Test Accuracy:{acc}, Test loss:{loss}\")\n",
    "\n",
    "loss, acc = evaluate(model, val_loader, error)\n",
    "print(f\"Validation Accuracy:{acc}, Validation loss:{loss}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMqKkQ7ZEHQBl2C2+rUUilU",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
